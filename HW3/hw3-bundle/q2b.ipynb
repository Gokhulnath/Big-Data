{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dERgPFyXXgto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea035b8-9fdf-4d54-82ce-7aacf10f6aca"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 39.6 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8W0jZqKXjCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e19d04-dd01-43ff-d6ba-661f8f3b2adb"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2ExXtfUXv83"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry, DenseMatrix"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCQK0ur5X1ES"
      },
      "source": [
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6stP5iUpX3HD"
      },
      "source": [
        "small_data = sc.textFile('/content/graph-small.txt')\n",
        "full_data = sc.textFile('/content/graph-full.txt')\n",
        "\n",
        "LAMBDA = 1\n",
        "NU = 1\n",
        "source_dest_pair = full_data.map(lambda x: (int(x.split('\\t')[0]) - 1, int(x.split('\\t')[1]) - 1)).distinct()\n",
        "edges = source_dest_pair.map(lambda x: (x[0], x[1], 1))\n",
        "edges_transpose = source_dest_pair.map(lambda x: (x[1], x[0], 1))\n",
        "L = CoordinateMatrix(edges).toBlockMatrix()\n",
        "L_transpose = CoordinateMatrix(edges_transpose).toBlockMatrix()\n",
        "\n",
        "h_init = []\n",
        "\n",
        "for i in range(1000):\n",
        "  h_init.append((i, 0, 1))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_52mKzKZIjr"
      },
      "source": [
        "h = CoordinateMatrix(sc.parallelize(h_init)).toBlockMatrix()\n",
        "a = None\n",
        "\n",
        "for i in range(40):\n",
        "\n",
        "  a_new = L_transpose.multiply(h)\n",
        "  a_new_max = np.max(np.array(a_new.toLocalMatrix().toArray()))\n",
        "  a_new_max_inverse = []\n",
        "  for j in range(1000):\n",
        "    a_new_max_inverse.append((j, j, 1 / a_new_max))\n",
        "  a_new_max_inverse = CoordinateMatrix(sc.parallelize(a_new_max_inverse)).toBlockMatrix()\n",
        "  a = a_new_max_inverse.multiply(a_new)\n",
        "\n",
        "  h_new = L.multiply(a)\n",
        "  h_new_max = np.max(np.array(h_new.toLocalMatrix().toArray()))\n",
        "  h_new_max_inverse = []\n",
        "  for j in range(1000):\n",
        "    h_new_max_inverse.append((j, j, 1 / h_new_max))\n",
        "  h_new_max_inverse = CoordinateMatrix(sc.parallelize(h_new_max_inverse)).toBlockMatrix()\n",
        "  h = h_new_max_inverse.multiply(h_new)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tscFZmqJZ4WN"
      },
      "source": [
        "h_numpy = np.array(h.toLocalMatrix().toArray())\n",
        "a_numpy = np.array(a.toLocalMatrix().toArray())\n",
        "h_min_args = np.argsort(h_numpy, axis = 0)[:5]\n",
        "a_min_args = np.argsort(a_numpy, axis = 0)[:5]\n",
        "h_max_args = np.argsort(-h_numpy, axis = 0)[:5]\n",
        "a_max_args = np.argsort(-a_numpy, axis = 0)[:5]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyRbEXAqd2vI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d11a61-5413-4113-9819-0c2c1c69b4c5"
      },
      "source": [
        "print(\"The 5 node ids with the highest hubbiness scores:\")\n",
        "for args in h_max_args:\n",
        "  print(\"Node id: {}, hubbiness score: {}\".format(args[0] + 1, h_numpy[args][0][0]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 5 node ids with the highest hubbiness scores:\n",
            "Node id: 840, hubbiness score: 1.0\n",
            "Node id: 155, hubbiness score: 0.9499618624906543\n",
            "Node id: 234, hubbiness score: 0.8986645288972264\n",
            "Node id: 389, hubbiness score: 0.863417110184379\n",
            "Node id: 472, hubbiness score: 0.8632841092495217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The 5 node ids with the lowest hubbiness scores:\")\n",
        "for args in h_min_args:\n",
        "  print(\"Node id: {}, hubbiness score: {}\".format(args[0] + 1, h_numpy[args][0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byi_z6U8AES0",
        "outputId": "8a4d20d6-e0cd-4479-dfdb-aa16fa979415"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 5 node ids with the lowest hubbiness scores:\n",
            "Node id: 23, hubbiness score: 0.042066854890936534\n",
            "Node id: 835, hubbiness score: 0.05779059354433016\n",
            "Node id: 141, hubbiness score: 0.06453117646225179\n",
            "Node id: 539, hubbiness score: 0.06602659373418492\n",
            "Node id: 889, hubbiness score: 0.07678413939216454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET5doifWehWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b40710f-0ce5-46d8-c334-e0e9ae611fb8"
      },
      "source": [
        "print(\"The 5 node ids with the highest authority scores:\")\n",
        "for args in a_max_args:\n",
        "  print(\"Node id: {}, hubbiness score: {}\".format(args[0] + 1, a_numpy[args][0][0]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 5 node ids with the highest authority scores:\n",
            "Node id: 893, hubbiness score: 1.0\n",
            "Node id: 16, hubbiness score: 0.9635572849634398\n",
            "Node id: 799, hubbiness score: 0.9510158161074016\n",
            "Node id: 146, hubbiness score: 0.9246703586198444\n",
            "Node id: 473, hubbiness score: 0.899866197360405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The 5 node ids with the lowest authority scores:\")\n",
        "for args in a_min_args:\n",
        "  print(\"Node id: {}, hubbiness score: {}\".format(args[0] + 1, a_numpy[args][0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZLhds0CAIeD",
        "outputId": "a64cab90-acc9-42af-a53d-c73b9ef9658b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 5 node ids with the lowest authority scores:\n",
            "Node id: 19, hubbiness score: 0.05608316377607618\n",
            "Node id: 135, hubbiness score: 0.06653910487622794\n",
            "Node id: 462, hubbiness score: 0.07544228624641902\n",
            "Node id: 24, hubbiness score: 0.08171239406816946\n",
            "Node id: 910, hubbiness score: 0.08571673456144878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "W-KD-IbLAOad"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}